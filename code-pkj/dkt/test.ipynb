{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfcb5077-46af-4e73-bb27-2f986db3036a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkyjun\u001b[0m (\u001b[33mrecsys_lvl2\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/opt/ml/input/code/dkt/wandb/run-20221121_061856-259uvr3l\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgiddy-music-223\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/recsys_lvl2/dkt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/recsys_lvl2/dkt/runs/259uvr3l\u001b[0m\n",
      "Start Training: Epoch 1\n",
      "Training steps: 0 Loss: 0.76302170753479\n",
      "Training steps: 50 Loss: 0.5541396737098694\n",
      "Training steps: 100 Loss: 0.5458081364631653\n",
      "Training steps: 150 Loss: 0.5975497961044312\n",
      "Training steps: 200 Loss: 0.5205838680267334\n",
      "Training steps: 250 Loss: 0.6013550758361816\n",
      "Training steps: 300 Loss: 0.6585097312927246\n",
      "TRAIN AUC : 0.7413979175204543 ACC : 0.6954720183715242\n",
      "VALID AUC : 0.7766698821775047 ACC : 0.740814696485623\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 2\n",
      "Training steps: 0 Loss: 0.31258049607276917\n",
      "Training steps: 50 Loss: 0.6731395125389099\n",
      "Training steps: 100 Loss: 0.44221603870391846\n",
      "Training steps: 150 Loss: 0.5197868347167969\n",
      "Training steps: 200 Loss: 0.5871422290802002\n",
      "Training steps: 250 Loss: 0.522391676902771\n",
      "Training steps: 300 Loss: 0.5782134532928467\n",
      "TRAIN AUC : 0.7670749374725725 ACC : 0.7135440067894763\n",
      "VALID AUC : 0.7761686837327447 ACC : 0.735223642172524\n",
      "\n",
      "Start Training: Epoch 3\n",
      "Training steps: 0 Loss: 0.46169519424438477\n",
      "Training steps: 50 Loss: 0.5885858535766602\n",
      "Training steps: 100 Loss: 0.5889779329299927\n",
      "Training steps: 150 Loss: 0.5887386798858643\n",
      "Training steps: 200 Loss: 0.5285168886184692\n",
      "Training steps: 250 Loss: 0.5487322211265564\n",
      "Training steps: 300 Loss: 0.5982730388641357\n",
      "TRAIN AUC : 0.776956959896512 ACC : 0.7214817033597923\n",
      "VALID AUC : 0.7783263897957379 ACC : 0.7382188498402555\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 4\n",
      "Training steps: 0 Loss: 0.4936230182647705\n",
      "Training steps: 50 Loss: 0.45899587869644165\n",
      "Training steps: 100 Loss: 0.5276942253112793\n",
      "Training steps: 150 Loss: 0.48831260204315186\n",
      "Training steps: 200 Loss: 0.5360609292984009\n",
      "Training steps: 250 Loss: 0.4806320369243622\n",
      "Training steps: 300 Loss: 0.5037346482276917\n",
      "TRAIN AUC : 0.7902850153279004 ACC : 0.731366382107733\n",
      "VALID AUC : 0.7884025089409455 ACC : 0.7470047923322684\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 5\n",
      "Training steps: 0 Loss: 0.3992173373699188\n",
      "Training steps: 50 Loss: 0.5420483350753784\n",
      "Training steps: 100 Loss: 0.4909009635448456\n",
      "Training steps: 150 Loss: 0.5539700388908386\n",
      "Training steps: 200 Loss: 0.5706470012664795\n",
      "Training steps: 250 Loss: 0.5405359268188477\n",
      "Training steps: 300 Loss: 0.6116092801094055\n",
      "TRAIN AUC : 0.8001749350455479 ACC : 0.7383056262792671\n",
      "VALID AUC : 0.78219932516397 ACC : 0.7362220447284346\n",
      "\n",
      "Start Training: Epoch 6\n",
      "Training steps: 0 Loss: 0.5993688106536865\n",
      "Training steps: 50 Loss: 0.3944392800331116\n",
      "Training steps: 100 Loss: 0.48987868428230286\n",
      "Training steps: 150 Loss: 0.4720938205718994\n",
      "Training steps: 200 Loss: 0.4224962592124939\n",
      "Training steps: 250 Loss: 0.4050920307636261\n",
      "Training steps: 300 Loss: 0.5026252865791321\n",
      "TRAIN AUC : 0.8127286087761457 ACC : 0.7443462632918976\n",
      "VALID AUC : 0.778552177630997 ACC : 0.7300319488817891\n",
      "\n",
      "Start Training: Epoch 7\n",
      "Training steps: 0 Loss: 0.5159950852394104\n",
      "Training steps: 50 Loss: 0.47553786635398865\n",
      "Training steps: 100 Loss: 0.48374143242836\n",
      "Training steps: 150 Loss: 0.44904041290283203\n",
      "Training steps: 200 Loss: 0.48394182324409485\n",
      "Training steps: 250 Loss: 0.5620381832122803\n",
      "Training steps: 300 Loss: 0.5412235260009766\n",
      "TRAIN AUC : 0.8271607815809622 ACC : 0.7547800908591683\n",
      "VALID AUC : 0.7730889545096487 ACC : 0.7304313099041534\n",
      "\n",
      "Start Training: Epoch 8\n",
      "Training steps: 0 Loss: 0.4663388431072235\n",
      "Training steps: 50 Loss: 0.4280893802642822\n",
      "Training steps: 100 Loss: 0.4156010150909424\n",
      "Training steps: 150 Loss: 0.6149483919143677\n",
      "Training steps: 200 Loss: 0.43292534351348877\n",
      "Training steps: 250 Loss: 0.5152134895324707\n",
      "Training steps: 300 Loss: 0.45210957527160645\n",
      "TRAIN AUC : 0.8421019617802953 ACC : 0.7700564125605311\n",
      "VALID AUC : 0.7730954416974527 ACC : 0.7180511182108626\n",
      "\n",
      "Start Training: Epoch 9\n",
      "Training steps: 0 Loss: 0.3861958086490631\n",
      "Training steps: 50 Loss: 0.43352407217025757\n",
      "Training steps: 100 Loss: 0.49837586283683777\n",
      "Training steps: 150 Loss: 0.3935180902481079\n",
      "Training steps: 200 Loss: 0.4507458508014679\n",
      "Training steps: 250 Loss: 0.5418373346328735\n",
      "Training steps: 300 Loss: 0.4983351230621338\n",
      "TRAIN AUC : 0.8581888910252126 ACC : 0.7799410913084719\n",
      "VALID AUC : 0.7780281476551343 ACC : 0.7310303514376997\n",
      "\n",
      "EarlyStopping counter: 5 out of 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            epoch ▁▂▃▄▅▅▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc_epoch ▁▂▃▄▅▅▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_auc_epoch ▁▃▃▄▅▅▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_epoch █▇▆▅▅▄▃▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  valid_acc_epoch ▇▅▆█▅▄▄▁▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  valid_auc_epoch ▃▂▃█▅▃▁▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            epoch 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc_epoch 0.77994\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_auc_epoch 0.85819\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_epoch 0.45171\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  valid_acc_epoch 0.73103\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  valid_auc_epoch 0.77803\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mgiddy-music-223\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/recsys_lvl2/dkt/runs/259uvr3l\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20221121_061856-259uvr3l/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python train.py --optimizer adamW --model attnlstm --file_name 100_20.csv --max_seq_len 100 --n_heads 16 --n_layers 1 --lr 0.0003 --patience 5 --drop_out 0.2 --n_epochs 10 --hidden_dim 100  --pos --hidden_size 496"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "447d6fbf-38a8-4269-ba3d-2a409ef9c7ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 100, 1]), torch.Size([64, 100, 496]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def feature_engineering(df):\n",
    "    \n",
    "    #유저별 시퀀스를 고려하기 위해 아래와 같이 정렬\n",
    "    df.sort_values(by=['userID','Timestamp'], inplace=True)\n",
    "    \n",
    "    #유저들의 문제 풀이수, 정답 수, 정답률을 시간순으로 누적해서 계산\n",
    "    df['user_correct_answer'] = df.groupby('userID')['answerCode'].transform(lambda x: x.cumsum().shift(1))\n",
    "    df['user_total_answer'] = df.groupby('userID')['answerCode'].cumcount()\n",
    "    df['user_acc'] = df['user_correct_answer']/df['user_total_answer']\n",
    "\n",
    "    # testId와 KnowledgeTag의 전체 정답률은 한번에 계산\n",
    "    # 아래 데이터는 제출용 데이터셋에 대해서도 재사용\n",
    "    correct_t = df.groupby(['testId'])['answerCode'].agg(['mean', 'sum'])\n",
    "    correct_t.columns = [\"test_mean\", 'test_sum']\n",
    "    correct_k = df.groupby(['KnowledgeTag'])['answerCode'].agg(['mean', 'sum'])\n",
    "    correct_k.columns = [\"tag_mean\", 'tag_sum']\n",
    "\n",
    "    df = pd.merge(df, correct_t, on=['testId'], how=\"left\")\n",
    "    df = pd.merge(df, correct_k, on=['KnowledgeTag'], how=\"left\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1971f32-a4d7-4c7a-8d5c-b769e658d72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.add_argument('--cate_feats', type=list, nargs=\"+\",\n",
    "                    default=[\"assessmentItemID\", 'testId',\"bigClass\",'KnowledgeTag','elapsedTimeClass2', 'bigClassAccCate',\n",
    "                    ],#'bigClass'],#'tagCount'],#'seenCount','year','month','day'],#, 'KTAccuracyCate'],\n",
    "                    help=\"['assessmentItemID', 'testId', 'KnowledgeTag','bigClass', 'KTAccuracyCate']\")\n",
    "\n",
    "### continous featurs\n",
    "parser.add_argument('--conti_feats', type=list, nargs=\"+\",\n",
    "                    default=['elapsed','cumAccuracy','bigClassAcc', \n",
    "                            'elo', 'recAccuracy',\n",
    "                            'testMean','tagMean','assessMean',\n",
    "                            'testStd','tagStd','assessStd', \n",
    "                            'tagCount',\n",
    "                            'tagLV', 'accuracy'\n",
    "                        #'elapsedTime', 'cumAccuracy','recAccuracy','bigClassAcc', 'elo'\n",
    "                    ], #'totalAnswer','cumAccuracy','recAccuracy','bigClassAcc', 'testMean', 'tagMean'],#'KnowledgeTagAcc'],\n",
    "                    help = \"['elapsedTime','recAccuracy',','cumAccuracy']\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
